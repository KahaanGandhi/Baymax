{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I implement a Multinomial Naive Bayes classifier on a public [spam email dataset](https://www.kaggle.com/datasets/shantanudhakadd/email-spam-detection-dataset-classification). \n",
    "\n",
    "In `NB-adult-census-income.ipynb`, we derived expressions for posterior probability and class-conditional probabilities by applying the Naive Bayes assumption to Bayes theorem. Modifying variables to represent the new challenge of classifying emails,\n",
    "\n",
    "$$ \n",
    "\\begin{align*}\n",
    "P(y=k|x) \\propto P(y=k) \\prod_{j=1}^n P(x_j | y = k) \\quad &\\Longrightarrow \\quad P(\\text{spam}|w_1, w_2, ... , w_n) \\propto P(\\text{spam}) \\prod_{i=1}^n P(w_i | \\text{spam}) \\\\\n",
    "P(x_j = v_j | y = k) = \\frac{N_{k,v_j} + \\alpha}{N_k + \\alpha \\times V_j} \\quad &\\Longrightarrow \\quad P(w_i | \\text{spam}) = \\frac{N_{w_i | \\text{spam}} + \\alpha}{N_{\\text{spam}} + \\alpha \\times N_\\text{vocabulary}}\n",
    "\\end{align*}\n",
    "$$\n",
    "* $N_{w_i | \\text{spam}}$: total number of times word $w_i$ occurs in spam messages\n",
    "* $N_{\\text{spam}}$: total number of words in all spam messages\n",
    "* $N_\\text{vocabulary}$: total number of words in vocabulary\n",
    "* $\\alpha$: Laplace smoothing parameter\n",
    "\n",
    "We will again store probabilities in log-space: \n",
    "$$\n",
    "\\hat{y} = \\underset{k \\in \\{1,2 \\}  }{\\text{argmax}} \\log P(y=k) + \\log \\sum_{i=1}^n P(w_i | y = k)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data = pd.read_csv(\"/Users/kahaan/Desktop/Torch/datasets/spam.csv\", encoding='ISO-8859-1')\n",
    "\n",
    "filtered_data = original_data.drop(columns=[\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"])  \n",
    "renamed_data = filtered_data.rename(columns={\"v1\": \"Label\", \"v2\": \"Text\"})\n",
    "shuffled_data = renamed_data.sample(frac=1)                 # Clean and shuffle dataset\n",
    "split_index = int(len(shuffled_data) * 0.8)                 # Split up into testing and training batches                                               \n",
    "training_data = shuffled_data[:split_index]\n",
    "testing_data = shuffled_data[split_index:]\n",
    "\n",
    "#print(renamed_data[\"Label\"].value_counts(normalize=True))  # Confirm that both batches reflect original\n",
    "#print(training_data[\"Label\"].value_counts(normalize=True))\n",
    "#print(testing_data[\"Label\"].value_counts(normalize=True))\n",
    "\n",
    "cleaned_training_data = training_data.copy()                # Further clean training set\n",
    "cleaned_training_data['Text'] = training_data['Text'].str.replace('[^a-zA-Z ]', '', regex=True).str.lower()\n",
    "cleaned_training_data['Text'] = cleaned_training_data['Text'].str.split()\n",
    "\n",
    "N_ham = shuffled_data[\"Label\"].value_counts()[0]            # Calculate class priors\n",
    "N_spam = shuffled_data[\"Label\"].value_counts()[1]\n",
    "alpha = 1                                                   # Laplacian smoothing\n",
    "number_of_classes = 2\n",
    "n = len(shuffled_data)\n",
    "P_ham = (N_ham + 1) / (n + number_of_classes)\n",
    "P_spam = (N_spam + 1) / (n + number_of_classes)\n",
    "log_priors = {'ham': np.log(P_ham),                         # Store class priors in log-space\n",
    "              'spam': np.log(P_spam)}\n",
    "\n",
    "vocabulary = set()                                          # Count unique words in training data\n",
    "for text in cleaned_training_data['Text']:\n",
    "    for word in text:\n",
    "        vocabulary.add(word)\n",
    "vocabulary = list(vocabulary)\n",
    "\n",
    "word_frequency = {}                                         # Use vocab to create word-counting structure\n",
    "for key in {'ham', 'spam'}:\n",
    "    if not key in word_frequency:\n",
    "        word_frequency[key] = {}\n",
    "        for word in vocabulary:                                     \n",
    "            word_frequency[key][word] = 0\n",
    "\n",
    "spam_messages = cleaned_training_data[cleaned_training_data['Label'] == 'spam']\n",
    "ham_messages = cleaned_training_data[cleaned_training_data['Label'] == 'ham']\n",
    "\n",
    "words_in_spam, words_in_ham = 0, 0                   # Count frequency of words for each class\n",
    "for text in spam_messages['Text']:\n",
    "    for word in text:\n",
    "        word_frequency['spam'][word] += 1\n",
    "        words_in_spam += 1\n",
    "\n",
    "for text in ham_messages['Text']:\n",
    "    for word in text:\n",
    "        word_frequency['ham'][word] += 1\n",
    "        words_in_ham += 1\n",
    "        \n",
    "\n",
    "word_log_prob = {}                                    # Create a structure to calculate conditional probability\n",
    "alpha = 1\n",
    "N_vocab = len(vocabulary)\n",
    "for key in word_frequency.keys():                     # Calculate conditional probalities of word given class\n",
    "    if not key in word_log_prob:\n",
    "        word_log_prob[key] = {}\n",
    "    for word in vocabulary:\n",
    "        if key == \"spam\":\n",
    "            N_words = words_in_spam\n",
    "        else:\n",
    "            N_words = words_in_ham\n",
    "        N_wi_label = word_frequency[key][word]\n",
    "        prob = (N_wi_label + alpha) / (N_words + alpha * N_vocab)\n",
    "        word_log_prob[key][word] = np.log(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(message:str):                                    # Classify a new message\n",
    "    message = re.sub('[^a-zA-Z]', ' ', message).lower()       # Clean new message\n",
    "    words = message.split()\n",
    "    \n",
    "    class_probabilities = {}\n",
    "    for class_label in log_priors.keys():                     # For each potential class...\n",
    "        log_prob = log_priors[class_label]                    # begin with class prior\n",
    "        for word in words:                                    # For each word...\n",
    "            if word in vocabulary:                            # if it was in training vocab...             \n",
    "                log_prob += word_log_prob[class_label][word]  # add conditional probability\n",
    "            else:                                             # Otherwise, calculate it now\n",
    "                if key == \"spam\":\n",
    "                    N_words = words_in_spam\n",
    "                else:\n",
    "                    N_words = words_in_ham\n",
    "                prob = (1 + alpha) / (N_words + alpha * len(vocabulary))\n",
    "                log_prob += prob\n",
    "        class_probabilities[class_label] = log_prob           # Store probabilities, to be argmax'd\n",
    "    most_likely_class = max(class_probabilities, key=class_probabilities.get)\n",
    "    return most_likely_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy rate: 99.1 %\n"
     ]
    }
   ],
   "source": [
    "correct_labels = 0                                                  # Test classifier on training data\n",
    "for i in range(len(testing_data)):\n",
    "    text = testing_data.iloc[i][1]\n",
    "    if classify(text) == testing_data.iloc[i][0]:\n",
    "        correct_labels += 1\n",
    "print(\"Accuracy rate:\",round(correct_labels/(len(testing_data)),4) * 100, \"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
