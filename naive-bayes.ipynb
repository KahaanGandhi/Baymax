{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayes theorem states that \n",
    "$$\n",
    "P(A | B) = \\frac{P(A \\cap Y)}{P(B)} = \\frac{P(B|A) P(A)}{P(B)}\\quad \\Longrightarrow \\quad \\text{posterior} = \\frac{\\text{Class likelihood} \\times \\text{prior}}{\\text{Evidence}}\n",
    "$$\n",
    "* $P(A \\cap Y)$: the probability of $A$ and $B$\n",
    "* $P(A|B)$: the probability of $A$ given $B$\n",
    "* $P(B|A)$: the probability of $B$ given $A$\n",
    "* P(A): the probability of $A$ occuring\n",
    "* P(B): the probability of $B$ occuring\n",
    "\n",
    "And, when two events are independent, \n",
    "$$\n",
    "P(A \\cap B) = P(A) \\cdot P(B)\n",
    "$$\n",
    "\n",
    "For class variable $y$ and dependent feature vector $X$, we can apply Bayes theorem: \n",
    "$$\n",
    "P(y|X) = \\frac{P(X|y)P(y)}{P(X)}, \\text{where } X = (x_1, x_2, x_3,...,x_n)\n",
    "$$\n",
    "\n",
    "The Naive Bayes approximation assumes that different feature dimensions (elements of $X$) are are conditionally independent. Applying this to our posterior probability: \n",
    "$$\n",
    "P(y|x_1,...,x_n) = \\frac{P(x_1|y)P(x_2|y)...P(x_3|y)P(y)}{P(x_1)P(x_2)...P(x_n)} \n",
    "$$\n",
    "$$\n",
    "P(y|x_1,...,x_n) \\propto P(y) \\prod_{j=1}^n P(x_j |y)\n",
    "$$\n",
    "For class label k, \n",
    "$$\n",
    "P(y=k|x) \\propto P(y=k) \\prod_{j=1}^n P(x_j | y = k)\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must now calculate model parameters ($\\theta$'s) for each class probability $P(y = k)$ and each conditional-class probability $p(x_j = v | y = k)$. Begining the the simpler case, \n",
    "$$\n",
    "\\theta_k = P(y = k) = \\frac{N_k + \\alpha}{n+ \\alpha \\times K}\n",
    "$$\n",
    "* $N_k$: number of instances with label $k$\n",
    "* $n$: number of training instances\n",
    "* $K$: number of unique classes\n",
    "* $\\alpha$: Laplace smoothing parameter\n",
    "\n",
    "To calculate class-conditional probabilities, \n",
    "$$\n",
    "\\theta_{k,j,v} = P(x_j = v_j | y = k) = \\frac{N_{k,v_j} + \\alpha}{N_k + \\alpha \\times V_j}\n",
    "$$\n",
    "* $N_{k,j.v}$: the number of times the value $v_j$ occurs in feature $x_j$ in training instances where the the target class is $k$\n",
    "* $N_k$: the total count of all feature values where the target class is $k$\n",
    "* $V_{j}$: the number distinct values of distinct values that feature $x_j$ can take\n",
    "* $\\alpha$: Laplace smoothing parameter\n",
    "  \n",
    "By setting $\\alpha = 1$, we will apply Laplace smoothing to handle zero-frequency problems (when a word has not been observed in a class). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, to make an inference/prediction about a new instance, we can define a Naive Bayes classifier by modifying our probability calculation for log space:\n",
    "$$\n",
    "P(y=k|x) \\propto P(y=k) \\prod_{j=1}^n P(x_j | y = k) \n",
    "$$\n",
    "$$\n",
    "\\hat{y} = \\underset{k \\isin \\{1,2,...,K \\}  }{\\text{argmax}} P(y=k) \\prod_{j=1}^n P(x_j | y = k) \n",
    "$$\n",
    "$$\\boxed{\\boxed{\n",
    "\\hat{y} = \\underset{k \\isin \\{1,2,...,K \\}  }{\\text{argmax}} \\log P(y=k) + \\log \\sum_{j=1}^n P(x_j | y = k) }}\n",
    "$$\n",
    "If we instead had a dataset of continous (not discrete) variables, we can apply a similar classification method: Gaussian Naive Bayes. To do this, we must first assume that each feature is normally (Gaussian) distributed. For each class, we can then calculate mean and variance of each feature. That is, for a dataset with features $x_1, x_2,...,x_n$ and classes $k_1, k_2,...,k_m$, for each feature $x_i$ in class $k_j$, we calculate mean $\\mu_{ij}$ and variance $\\sigma_{ij}^2$. For example, mean is the sum of all values of feature $x_i$ for the instances in class $k_j$ divided by the number of instances of in class $k_j$, These are calculated as:\n",
    "$$\n",
    "\\mu_{ij} = \\frac{\\sum_{x \\isin k_j} x_i }{N_{c_j}} \\quad \\text{and} \\quad \\sigma_{ij} = \\frac{\\sum_{x \\isin k_j} \\left( x_i - \\mu_{ij} \\right)^2 }{N_{c_j}} \n",
    "$$\n",
    "We can then use the Gaussian probability density function to calculate the probability of observing the specific value $x$ for feature $x_i$ given that it belongs to class $k_j$. \n",
    "$$\n",
    "P(x_i = x | k_j) = \\frac{1}{\\sqrt{2 \\pi \\sigma_{ij}^2}} \\exp \\left(- \\frac{\\left( x - \\mu_{ij} \\right)^2}{2\\sigma_{ij}^2}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "census_data = pd.read_csv(\"./datasets/1994_census_cleaned_train.csv\")\n",
    "\n",
    "target_class = \"sex\"\n",
    "alpha = 1\n",
    "\n",
    "class_frequencies = {}\n",
    "training_instances = 0\n",
    "for i in range(len(census_data)):\n",
    "    training_instances += 1\n",
    "    if census_data[target_class][i] in class_frequencies:\n",
    "        class_frequencies[census_data[target_class][i]] += 1\n",
    "    else:\n",
    "        class_frequencies[census_data[target_class][i]] = 1\n",
    "\n",
    "log_priors = {}\n",
    "for key in class_frequencies:\n",
    "    theta = (class_frequencies[key] + alpha) / (training_instances + alpha * len(class_frequencies))\n",
    "    log_priors[key] = np.log(theta)\n",
    "\n",
    "feature_frequencies = {}\n",
    "\n",
    "for key in class_frequencies:\n",
    "    feature_frequencies[key] = {}\n",
    "\n",
    "column_headers = list(census_data.columns.values)\n",
    "for i in range(len(census_data)):\n",
    "    for j in range(len(column_headers)):\n",
    "        if column_headers[j] != target_class:\n",
    "            if census_data[column_headers[j]][i] in feature_frequencies[census_data[target_class][i]]:\n",
    "                feature_frequencies[census_data[target_class][i]][census_data[column_headers[j]][i]] += 1\n",
    "            else:\n",
    "                feature_frequencies[census_data[target_class][i]][census_data[column_headers[j]][i]] = 1\n",
    "                \n",
    "feature_space_size = {}\n",
    "for j in range(len(column_headers)):\n",
    "    seen = []\n",
    "    for i in range(len(census_data)):\n",
    "        if census_data[column_headers[j]][i] not in seen:\n",
    "            seen.append(census_data[column_headers[j]][i])\n",
    "    feature_space_size[column_headers[j]] = len(seen)\n",
    "    seen = []\n",
    "    \n",
    "class_conditional_probs = {}\n",
    "\n",
    "# Initialize the class_conditional_probs structure\n",
    "for class_value in class_frequencies.keys():\n",
    "    class_conditional_probs[class_value] = {feature: {} for feature in census_data.columns if feature != target_class}\n",
    "\n",
    "# Calculate class-conditional probabilities\n",
    "for feature in census_data.columns:\n",
    "    if feature != target_class:\n",
    "        for class_value in class_frequencies.keys():\n",
    "            total_count = sum(census_data[census_data[target_class] == class_value][feature].value_counts().values)\n",
    "            V_j = feature_space_size[feature]\n",
    "            for feature_value in census_data[feature].unique():\n",
    "                N_k_vj = census_data[(census_data[target_class] == class_value) & (census_data[feature] == feature_value)].shape[0]\n",
    "                theta_k_j_v = (N_k_vj + alpha) / (total_count + alpha * V_j)\n",
    "                class_conditional_probs[class_value][feature][feature_value] = np.log(theta_k_j_v)\n",
    "                \n",
    "features = [col for col in census_data.columns if col != target_class]\n",
    "\n",
    "def classify_NB(new_instance, log_priors, class_conditional_probs, features):\n",
    "    class_probabilities = {}\n",
    "    \n",
    "    for class_value in log_priors.keys():\n",
    "        log_prob = log_priors[class_value]\n",
    "        for feature in features:\n",
    "            feature_value = new_instance.get(feature)\n",
    "            if feature_value and feature_value in class_conditional_probs[class_value][feature]:\n",
    "                log_prob += class_conditional_probs[class_value][feature][feature_value]\n",
    "        else:\n",
    "            pass\n",
    "        class_probabilities[class_value] = log_prob\n",
    "    most_likely_class = max(class_probabilities, key=class_probabilities.get)\n",
    "    return most_likely_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Female'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_instance = {\n",
    "    'age': 'Senior',\n",
    "    'workclass': 'Private',\n",
    "    'education': 'HS-grad',\n",
    "    'marital_status': 'Widowed',\n",
    "    'occupation': 'Exec-managerial',\n",
    "    'relationship': 'Not-in-family',\n",
    "    'race': 'White',\n",
    "    'sex': 'Female',\n",
    "    'hours_per_week': 'Part-time',\n",
    "    'income': '<=50K'\n",
    "}\n",
    "\n",
    "classify_NB(new_instance, log_priors, class_conditional_probs, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
