{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I implement a Multinomial Naive Bayes classifier on a public [Adult Census Income](https://www.kaggle.com/datasets/uciml/adult-census-income) dataset from 1994, reviewing the relavent Bayesian methods behind the algorithm. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayes theorem states that \n",
    "$$\n",
    "P(A | B) = \\frac{P(A \\cap Y)}{P(B)} = \\frac{P(B|A) P(A)}{P(B)}\\quad \\Longleftrightarrow \\quad \\text{posterior} = \\frac{\\text{Class likelihood} \\times \\text{prior}}{\\text{Evidence}}\n",
    "$$\n",
    "* $P(A \\cap Y)$: the probability of $A$ and $B$\n",
    "* $P(A|B)$: the probability of $A$ given $B$\n",
    "* $P(B|A)$: the probability of $B$ given $A$\n",
    "* P(A): the probability of $A$ occuring\n",
    "* P(B): the probability of $B$ occuring\n",
    "\n",
    "And, when two events are independent, \n",
    "$$\n",
    "P(A \\cap B) = P(A) \\cdot P(B)\n",
    "$$\n",
    "\n",
    "For class variable $y$ and dependent feature vector $X$, we can apply Bayes theorem: \n",
    "$$\n",
    "P(y|X) = \\frac{P(X|y)P(y)}{P(X)}, \\text{where } X = (x_1, x_2, x_3,...,x_n)\n",
    "$$\n",
    "\n",
    "The Naive Bayes approximation assumes that different feature dimensions (elements of $X$) are are conditionally independent. Applying this to our posterior probability: \n",
    "$$\n",
    "P(y|x_1,...,x_n) = \\frac{P(x_1|y)P(x_2|y)...P(x_3|y)P(y)}{P(x_1)P(x_2)...P(x_n)} \n",
    "$$\n",
    "$$\n",
    "P(y|x_1,...,x_n) \\propto P(y) \\prod_{j=1}^n P(x_j |y)\n",
    "$$\n",
    "For class label k, \n",
    "$$\n",
    "P(y=k|x) \\propto P(y=k) \\prod_{j=1}^n P(x_j | y = k)\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must now calculate model parameters ($\\theta$'s) for each class probability $P(y = k)$ and each conditional-class probability $p(x_j = v | y = k)$. Begining the the simpler case, \n",
    "$$\n",
    "\\theta_k = P(y = k) = \\frac{N_k + \\alpha}{n+ \\alpha \\times K}\n",
    "$$\n",
    "* $N_k$: number of instances with label $k$\n",
    "* $n$: number of training instances\n",
    "* $K$: number of unique classes\n",
    "* $\\alpha$: Laplace smoothing parameter\n",
    "\n",
    "To calculate class-conditional probabilities, \n",
    "$$\n",
    "\\theta_{k,j,v} = P(x_j = v_j | y = k) = \\frac{N_{k,v_j} + \\alpha}{N_k + \\alpha \\times V_j}\n",
    "$$\n",
    "* $N_{k,j.v}$: the number of times the value $v_j$ occurs in feature $x_j$ in training instances where the the target class is $k$\n",
    "* $N_k$: the total count of all feature values where the target class is $k$\n",
    "* $V_{j}$: the number distinct values of distinct values that feature $x_j$ can take\n",
    "* $\\alpha$: Laplace smoothing parameter\n",
    "  \n",
    "By setting $\\alpha = 1$, we will apply Laplace smoothing to handle zero-frequency problems (when a word has not been observed in a class). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, to make an inference/prediction about a new instance, we can define a Naive Bayes classifier by modifying our probability calculation for log space:\n",
    "$$\n",
    "P(y=k|x) \\propto P(y=k) \\prod_{j=1}^n P(x_j | y = k) \n",
    "$$\n",
    "$$\n",
    "\\hat{y} = \\underset{k \\in \\{1,2,...,K \\}  }{\\text{argmax}} P(y=k) \\prod_{j=1}^n P(x_j | y = k) \n",
    "$$\n",
    "$$\\boxed{\\boxed{\n",
    "\\hat{y} = \\underset{k \\in \\{1,2,...,K \\}  }{\\text{argmax}} \\log P(y=k) + \\log \\sum_{j=1}^n P(x_j | y = k) }}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier:\n",
    "    def __init__(self, alpha=1):                                    # Initialize an untrained NB classifier\n",
    "        self.alpha = alpha                                          # Laplacian smoothing as default\n",
    "        self.log_priors = None\n",
    "        self.class_conditional_probs = None\n",
    "        self.features = None\n",
    "\n",
    "    def train(self, data, target_class):\n",
    "        \"\"\"\n",
    "        Calculates priors \n",
    "        # Calculating class frequencies and log priors\"\"\"\n",
    "        training_instances = len(data)                              # Count the number of instances in the training data\n",
    "        class_frequencies = {}                                      # Calculate the frequency of each class in target column (helpful for priors)\n",
    "        for i in range(training_instances):\n",
    "            class_label = data.iloc[i][target_class]\n",
    "            if class_label in class_frequencies:\n",
    "                class_frequencies[class_label] += 1\n",
    "            else:\n",
    "                class_frequencies[class_label] = 1\n",
    "        self.log_priors = {}                                        # Calculate log of prior probabilities for each class\n",
    "        for class_label, freq in class_frequencies.items():\n",
    "            theta = (freq + self.alpha) / (training_instances + self.alpha * len(class_frequencies))\n",
    "            self.log_priors[class_label] = np.log(theta)\n",
    "\n",
    "        self.class_conditional_probs = {}                           # Initialize structure to store class-conditional probabilities\n",
    "        for class_value in class_frequencies.keys():                # Iterate over every feature for every class, skipping target class\n",
    "            self.class_conditional_probs[class_value] = {}\n",
    "            for feature in data.columns:                            # Count instances of this feature for the current class\n",
    "                if feature != target_class:\n",
    "                    total_count = 0\n",
    "                    for feature_value in data[feature].unique():\n",
    "                        total_count += len(data[(data[target_class] == class_value) & (data[feature] == feature_value)])\n",
    "\n",
    "                    V_j = len(data[feature].unique())                                           # Count how many unique values (options) for this feature\n",
    "                    for feature_value in data[feature].unique():                                # Now, iterating over each unique value of the feature\n",
    "                        N_k_vj = len(data[(data[target_class] == class_value) & (data[feature] == feature_value)])\n",
    "                        theta_k_j_v = (N_k_vj + self.alpha) / (total_count + self.alpha * V_j)\n",
    "                        if feature not in self.class_conditional_probs[class_value]:            # Log-tranform probability and store it in dictionary\n",
    "                            self.class_conditional_probs[class_value][feature] = {}\n",
    "                        self.class_conditional_probs[class_value][feature][feature_value] = np.log(theta_k_j_v)\n",
    "\n",
    "        self.features = [feature for feature in data.columns if feature != target_class]        # List of columns that will be used to train\n",
    "\n",
    "    def predict(self, new_instance):\n",
    "        class_probabilities = {}                                        # Initialize structure to store probability of new instance being each class\n",
    "        for class_label in self.log_priors.keys():                      # Use each prior as initial guess...\n",
    "            log_prob = self.log_priors[class_label]\n",
    "            for feature in self.features:                               # add the conditional probability of each feature given the current class...\n",
    "                feature_value = new_instance.get(feature)\n",
    "                log_prob += self.class_conditional_probs[class_label][feature][feature_value]\n",
    "            class_probabilities[class_label] = log_prob\n",
    "\n",
    "        return max(class_probabilities, key=class_probabilities.get)    # and return class which yields the highest probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy rate: 0.82 %\n"
     ]
    }
   ],
   "source": [
    "classifier = NaiveBayesClassifier()\n",
    "\n",
    "train_data = pd.read_csv(\"./datasets/1994_census_cleaned_train.csv\")    # Training NB classifier on 1994 Adult Census Income\n",
    "target_class = \"sex\"\n",
    "classifier.train(train_data, target_class)\n",
    "\n",
    "test_data = pd.read_csv(\"./datasets/1994_census_cleaned_test.csv\")      # Testing it on a different unique batch \n",
    "\n",
    "new_instance = {}\n",
    "correct_labels, incorrect_labels = 0, 0\n",
    "for i in range(len(test_data)):\n",
    "    for j in range(len(test_data.columns)):\n",
    "        if test_data.columns[j] != target_class:\n",
    "            new_instance[test_data.columns[j]] = test_data.iloc[i][j]\n",
    "        else:\n",
    "            correct_value = test_data[target_class][i]\n",
    "    if classifier.predict(new_instance) == correct_value:\n",
    "        correct_labels += 1\n",
    "    else:\n",
    "        incorrect_labels += 1\n",
    "\n",
    "print(\"Accuracy rate:\", round((correct_labels / (correct_labels + incorrect_labels)), 2), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we instead had a dataset of continous (not discrete) variables, we can apply a similar classification method: Gaussian Naive Bayes. To do this, we must first assume that each feature is normally (Gaussian) distributed. For each class, we can then calculate mean and variance of each feature. That is, for a dataset with features $x_1, x_2,...,x_n$ and classes $k_1, k_2,...,k_m$, for each feature $x_i$ in class $k_j$, we calculate mean $\\mu_{ij}$ and variance $\\sigma_{ij}^2$. For example, mean is the sum of all values of feature $x_i$ for the instances in class $k_j$ divided by the number of instances of in class $k_j$, These are calculated as:\n",
    "$$\n",
    "\\mu_{ij} = \\frac{\\sum_{x \\in k_j} x_i }{N_{c_j}} \\quad \\text{and} \\quad \\sigma_{ij} = \\frac{\\sum_{x \\in k_j} \\left( x_i - \\mu_{ij} \\right)^2 }{N_{c_j}} \n",
    "$$\n",
    "We can then use the Gaussian probability density function to calculate the probability of observing the specific value $x$ for feature $x_i$ given that it belongs to class $k_j$. \n",
    "$$\n",
    "P(x_i = x | k_j) = \\frac{1}{\\sqrt{2 \\pi \\sigma_{ij}^2}} \\exp \\left(- \\frac{\\left( x - \\mu_{ij} \\right)^2}{2\\sigma_{ij}^2}\\right)\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
